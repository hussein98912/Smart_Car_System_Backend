import cv2
import torch
from torchvision import transforms

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† PyTorch
model_path = r"C:\Users\slman\Desktop\smart_car_backend\front_camera_detection\models\resnet_model.pt"
model = torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)
model.eval()

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
def process_video():
    input_path = r'media/processed2_output.mp4'
    output_path = r'media/processed3_output.mp4'

    cap = cv2.VideoCapture(input_path)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    preprocess = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),  # Ø­Ø³Ø¨ Ù…Ø§ ÙŠØ­ØªØ§Ø¬Ù‡ Ù†Ù…ÙˆØ°Ø¬Ùƒ
        transforms.ToTensor()
    ])

    frame_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        input_tensor = preprocess(frame).unsqueeze(0)
        with torch.no_grad():
            outputs = model(input_tensor)

        # Ø§ÙØªØ±Ø¶ Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¹Ø·ÙŠ class ÙÙ‚Ø· (Ø£Ø¶Ù logic Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬)
        predicted_class = torch.argmax(outputs, dim=1).item()

        label = f"Class: {predicted_class}"
        cv2.putText(frame, label, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        out.write(frame)
        frame_count += 1
        print(f"ğŸŸ¢ ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙØ±ÙŠÙ… {frame_count}")

    cap.release()
    out.release()
    print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
    return output_path

if __name__ == '__main__':
    output_path = process_video()
    print(f"ğŸ“ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù†Ø§ØªØ¬: {output_path}")
